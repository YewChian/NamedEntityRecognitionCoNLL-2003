{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d17ea9",
   "metadata": {},
   "source": [
    "# Part 1. Sequence Tagging: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190dbd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\r\n",
      "Version: 4.3.2\r\n",
      "Summary: Python framework for fast Vector Space Modelling\r\n",
      "Home-page: https://radimrehurek.com/gensim/\r\n",
      "Author: Radim Rehurek\r\n",
      "Author-email: me@radimrehurek.com\r\n",
      "License: LGPL-2.1-only\r\n",
      "Location: /Users/raghavrnair/opt/anaconda3/envs/CZ4045/lib/python3.9/site-packages\r\n",
      "Requires: numpy, scipy, smart-open\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c3be427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c289d",
   "metadata": {},
   "source": [
    "# Part 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef05596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e35203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610f8d3",
   "metadata": {},
   "source": [
    "## Question 1.1\n",
    "### Based on word2vec embeddings you have downloaded, use cosine similarity to find the most similar word to each of these words: (a) “student”; (b) “Apple”; (c) “apple”. Report the most similar word and its cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e02dd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Word\t\tMost similar word\tCosine similarity\n",
      "-----------------------------------------------------------------------\n",
      "student\t\tstudents  \t\t0.7294867038726807\n",
      "Apple\t\tApple_AAPL  \t\t0.7456986308097839\n",
      "apple\t\tapples  \t\t0.720359742641449\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "words = [\"student\", \"Apple\", \"apple\"]\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"Word\\t\\tMost similar word\\tCosine similarity\")\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "for word in words:\n",
    "    most_similar = embeddings.most_similar(positive=[word])\n",
    "    print(f\"{word}\\t\\t{most_similar[0][0]}  \\t\\t{most_similar[0][1]}\")\n",
    "print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88bc153",
   "metadata": {},
   "source": [
    "# Part 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f82193",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'eng.train'\n",
    "dev_dir = 'eng.testa'\n",
    "test_dir = 'eng.testb'\n",
    "\n",
    "def import_content(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            content = file.readlines()\n",
    "        file.close()\n",
    "    except Exception as e:\n",
    "        content = None\n",
    "        print(e)\n",
    "    \n",
    "    return content\n",
    "\n",
    "def print_items(item):\n",
    "    for s in item: print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c84498",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = import_content(train_dir)\n",
    "dev_content = import_content(dev_dir)\n",
    "test_content = import_content(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b43fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(content):\n",
    "    split_data = [c.split(' ') for c in content] if content != None else []\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    words = []\n",
    "\n",
    "    for line in split_data:\n",
    "        # if end of a sentence\n",
    "        if line == ['\\n']:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            s_text  = line[0]\n",
    "            s_tag = line[-1].replace('\\n','')\n",
    "\n",
    "            sentence.append([s_text, s_tag]) \n",
    "            words.append([s_text, s_tag])\n",
    "    \n",
    "    sentences.append(sentence)         \n",
    "\n",
    "    return sentences, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "126a4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_tag(sentences):\n",
    "    text = []\n",
    "    tag = []\n",
    "    combined = []\n",
    "    sentence_count = 1\n",
    "\n",
    "    for s in sentences:\n",
    "        for w in s:\n",
    "            w_text  = w[0]\n",
    "            w_tag = w[-1].replace('\\n','')\n",
    "\n",
    "            text.append(w_text)\n",
    "            tag.append(w_tag)        \n",
    "            combined.append({\n",
    "                'sentence': sentence_count,\n",
    "                'text' : w_text,\n",
    "                'tag' : w_tag\n",
    "            })   \n",
    "        sentence_count+=1       \n",
    "    return text, tag, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b5cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_words = split_sentences(train_content)\n",
    "dev_sentences, dev_words = split_sentences(dev_content)\n",
    "test_sentences, test_words = split_sentences(test_content)\n",
    "\n",
    "train_text, train_tag, train_combined = split_text_tag(train_sentences)\n",
    "dev_text, dev_tag, dev_combined = split_text_tag(dev_sentences)\n",
    "test_text, test_tag, test_combined = split_text_tag(test_sentences)\n",
    "\n",
    "train_voc = np.unique(np.array(train_text))\n",
    "dev_voc = np.unique(np.array(dev_text))\n",
    "tag_set = np.unique(np.array(train_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a64a2",
   "metadata": {},
   "source": [
    "# Question 1.2 a)\n",
    "### Describe the size (number of sentences) of the training, development and test file for CoNLL2003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39b16d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences (training): 14987\n",
      "Number of sentences (dev): 3466\n",
      "Number of sentences (test): 3684\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences (training):\", len(train_sentences))\n",
    "print(\"Number of sentences (dev):\", len(dev_sentences))\n",
    "print(\"Number of sentences (test):\", len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ea233",
   "metadata": {},
   "source": [
    "### Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO, etc.) you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb6830f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag set (BIO): ['B-LOC' 'B-MISC' 'B-ORG' 'I-LOC' 'I-MISC' 'I-ORG' 'I-PER' 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag set (BIO):\", tag_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08edc2d0",
   "metadata": {},
   "source": [
    "# Question 1.2 b)\n",
    "###  Choose an example sentence from the training set of CoNLL2003 that has at least two named entities with more than one word. Explain how to form complete named entities from the label for each word, and list all the named entities in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc2c3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_ne_sentence(sentences):\n",
    "    for sentence in sentences:\n",
    "        ne_count = 0\n",
    "        for word_info in sentence:\n",
    "            if \"B-\" in word_info[-1]:\n",
    "                ne_count+=1\n",
    "        if ne_count == 2:\n",
    "            return sentence\n",
    "    return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d5ae189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Swiss', 'I-MISC'],\n",
       " ['Grand', 'B-MISC'],\n",
       " ['Prix', 'I-MISC'],\n",
       " ['World', 'B-MISC'],\n",
       " ['Cup', 'I-MISC'],\n",
       " ['cycling', 'O'],\n",
       " ['race', 'O'],\n",
       " ['on', 'O'],\n",
       " ['Sunday', 'O'],\n",
       " [':', 'O']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = get_multiple_ne_sentence(train_sentences)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18b6ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities(sentence):\n",
    "    inside_tags = ['I-ORG', 'I-LOC', 'I-PER', 'I-MISC'] # Tags that require multiple words to form an entity\n",
    "    begin_tags = ['B-LOC', 'B-ORG', 'B-MISC'] # Tags that are single word entities\n",
    "    outside_tags = ['O']\n",
    "    entities = [] # all entities gotten from search\n",
    "    entity = [] # word group of current entity if any group tags encountered\n",
    "    \n",
    "    for c in sentence:\n",
    "        if (c['tag'] in begin_tags or c['tag'] in outside_tags or c['tag'] == '\\n') and len(entity) != 0:\n",
    "            entities.append(' '.join(entity))\n",
    "            entity = []\n",
    "        if c['tag'] in begin_tags or c['tag'] in inside_tags: \n",
    "            entity.append(c['text'])\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ed08fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete named entities in the sentence: ['Swiss', 'Grand Prix', 'World Cup']\n"
     ]
    }
   ],
   "source": [
    "_,_,sentence_text_tag = split_text_tag([sentence])\n",
    "print(\"Complete named entities in the sentence:\", get_named_entities(sentence_text_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb1a91",
   "metadata": {},
   "source": [
    "# Part 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5eb6ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_names = ['word', 'useless1', 'useless2', 'tag']\n",
    "train_raw = pd.read_csv(\"eng.train\", header=None, sep=' ', names=header_names, skip_blank_lines=False, quotechar=\"|\")\n",
    "validation_raw = pd.read_csv(\"eng.testa\", header=None, sep=' ', names=header_names, skip_blank_lines=False, quotechar=\"|\")\n",
    "test_raw = pd.read_csv(\"eng.testb\", header=None, sep=' ', names=header_names, skip_blank_lines=False, quotechar=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b37fa083",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_indices = train_raw[train_raw['word'].isnull()].reset_index()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2de5ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create average embedding to replace Out-Of-Vocabulary words\n",
    "average_embedding = np.mean(embeddings.vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52f09a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na_labels(data):\n",
    "  temp = data[['word', 'tag']]\n",
    "  #without_na = temp.dropna(subset=['tag']).reset_index().drop(columns='index')\n",
    "  return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b30f163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_to_integer_dict(train_without_na):\n",
    "  count = 0\n",
    "  tag_to_integer_dictionary = {}\n",
    "  for tag in train_without_na['tag'].unique():\n",
    "    tag_to_integer_dictionary[tag] = count\n",
    "    count += 1\n",
    "\n",
    "  return tag_to_integer_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6ce64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_to_integer_dict_from_raw(train_raw):\n",
    "  temp = train_raw[['word', 'tag']]\n",
    "  without_na = temp.dropna(subset=['tag']).reset_index().drop(columns='index')\n",
    "  tag_to_integer_dictionary = get_tag_to_integer_dict(without_na)\n",
    "  return tag_to_integer_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b2e8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_integer_dictionary = get_tag_to_integer_dict_from_raw(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2061a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_wordstags_array(train_without_na):\n",
    "  sentences_train = []\n",
    "  new_sentence = []\n",
    "\n",
    "  for i, row in train_without_na.iterrows():\n",
    "\n",
    "    if row.isna().all():\n",
    "      sentences_train.append(new_sentence)\n",
    "      new_sentence = []\n",
    "\n",
    "    else:\n",
    "      if len(new_sentence) >= 50:\n",
    "        # if sentence length is too long, break it up every 30 words, to prevent timesteps from being too large\n",
    "        new_sentence.append([row[0], row[1]])\n",
    "        sentences_train.append(new_sentence)\n",
    "        new_sentence = []\n",
    "\n",
    "      else:\n",
    "        new_sentence.append([row[0], row[1]])\n",
    "\n",
    "\n",
    "#    if i > 10000:\n",
    "#      break\n",
    "\n",
    "  return sentences_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ff1fd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_embeddingstags_array(sentences_wordstags_array, tag_to_integer_dictionary):\n",
    "  # replace words with embeddings and tags with integers\n",
    "  embeddings_in_sentences = []\n",
    "  new_sentence = []\n",
    "\n",
    "  count = 0\n",
    "  for sentence in sentences_wordstags_array:\n",
    "    for word, tag in sentence:\n",
    "      if tag not in tag_to_integer_dictionary:\n",
    "        #print(f\"{tag} not found with {word}, skipping\")\n",
    "        continue\n",
    "      if word not in embeddings:\n",
    "        #new_embedding = np.zeros(300)\n",
    "        new_embedding = average_embedding\n",
    "\n",
    "      else:\n",
    "        unnormalized_embedding = embeddings[word].astype(np.float32)\n",
    "        # Reshape the embedding to be a 2D array with a single row\n",
    "        embedding_reshaped = unnormalized_embedding.reshape(1, -1)\n",
    "        # Normalize the embedding\n",
    "        embedding_normalized = normalize(embedding_reshaped, axis=1, norm='l2')\n",
    "        # Flatten the normalized embedding back into a 1D array\n",
    "        new_embedding = embedding_normalized.flatten()\n",
    "\n",
    "      new_sentence.append([new_embedding, tag_to_integer_dictionary[tag]])\n",
    "      count += 1\n",
    "\n",
    "    embeddings_in_sentences.append(new_sentence)\n",
    "    new_sentence = []\n",
    "\n",
    "  return embeddings_in_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea2bd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_without_labels(sentences_embeddingstags):\n",
    "  # create copy that doesnt have the NER tag\n",
    "  without_label = []\n",
    "  new_sentence = []\n",
    "\n",
    "  count = 0\n",
    "  for sentence in sentences_embeddingstags:\n",
    "    for embedding, tag in sentence:\n",
    "      new_sentence.append(embedding)\n",
    "      count += 1\n",
    "\n",
    "    without_label.append(new_sentence)\n",
    "    new_sentence = []\n",
    "\n",
    "  return without_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa995c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(sentences_embeddingstags):\n",
    "  # create labels\n",
    "  labels = []\n",
    "  new_sentence = []\n",
    "\n",
    "  count = 0\n",
    "  for sentence in sentences_embeddingstags:\n",
    "    for embedding, tag in sentence:\n",
    "      new_sentence.append(tag)\n",
    "      count += 1\n",
    "\n",
    "    labels.append(np.array(new_sentence))\n",
    "    new_sentence = []\n",
    "\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da42a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_without_labels_and_labels(without_labels, labels, max_sentence_length):\n",
    "  # padding\n",
    "  print(\"padding: \", max_sentence_length)\n",
    "  padded = tf.keras.utils.pad_sequences(without_labels, padding=\"post\", dtype=\"float32\", maxlen=max_sentence_length, value=0)\n",
    "  padded_labels = tf.keras.utils.pad_sequences(labels, padding=\"post\", maxlen=max_sentence_length, value=999)\n",
    "\n",
    "  return padded, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffd61bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_padded_and_padded_labels_to_np_arrays(padded, padded_labels):\n",
    "  # input: [batch, timestep, feature]\n",
    "  padded_np = np.array(padded)\n",
    "  padded_labels_np = np.array(padded_labels)\n",
    "  \"\"\"\n",
    "  print(type(train_labels))\n",
    "  print(type(train_labels[0]))\n",
    "  print(type(train_labels[0][0]))\n",
    "  print(type(train))\n",
    "  print(type(train[0]))\n",
    "  print(type(train[0][0]))\n",
    "  print(type(train[0][0][0]))\n",
    "  \"\"\"\n",
    "  return padded_np, padded_labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "472f4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw_to_input_and_labels(raw, tag_to_integer_dictionary):\n",
    "  without_na = drop_na_labels(raw)\n",
    "  num_classes = len(tag_to_integer_dictionary)\n",
    "  sentences_wordstags = get_sentences_wordstags_array(without_na)\n",
    "  sentences_embeddingstags = get_sentences_embeddingstags_array(sentences_wordstags, tag_to_integer_dictionary)\n",
    "  without_labels = get_without_labels(sentences_embeddingstags)\n",
    "  labels = get_labels(sentences_embeddingstags)\n",
    "  \"\"\"\n",
    "  del without_na\n",
    "  del sentences_wordstags\n",
    "  del sentences_embeddingstags\n",
    "\n",
    "  gc.collect()\n",
    "  \"\"\"\n",
    "\n",
    "  return without_labels, labels, tag_to_integer_dictionary, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e82536d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_input_and_labels(without_labels, labels, max_sentence_length):\n",
    "  padded, padded_labels = pad_without_labels_and_labels(without_labels, labels, max_sentence_length)\n",
    "  padded_np, padded_labels_np = convert_padded_and_padded_labels_to_np_arrays(padded, padded_labels)\n",
    "\n",
    "  return padded_np, padded_labels_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0b147d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qv/zhjqscls3zz__9246tdxr26r0000gn/T/ipykernel_71657/185571429.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_sentence.append([row[0], row[1]])\n",
      "/var/folders/qv/zhjqscls3zz__9246tdxr26r0000gn/T/ipykernel_71657/185571429.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_sentence.append([row[0], row[1]])\n",
      "/var/folders/qv/zhjqscls3zz__9246tdxr26r0000gn/T/ipykernel_71657/185571429.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_sentence.append([row[0], row[1]])\n",
      "/var/folders/qv/zhjqscls3zz__9246tdxr26r0000gn/T/ipykernel_71657/185571429.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_sentence.append([row[0], row[1]])\n",
      "/var/folders/qv/zhjqscls3zz__9246tdxr26r0000gn/T/ipykernel_71657/185571429.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_sentence.append([row[0], row[1]])\n",
      "/var/folders/qv/zhjqscls3zz__9246tdxr26r0000gn/T/ipykernel_71657/185571429.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  new_sentence.append([row[0], row[1]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding:  51\n",
      "padding:  51\n",
      "padding:  51\n"
     ]
    }
   ],
   "source": [
    "unpadded_train, unpadded_train_labels, tag_to_integer_dictionary, num_classes = process_raw_to_input_and_labels(train_raw, tag_to_integer_dictionary)\n",
    "unpadded_val, unpadded_val_labels, unused_val_dictionary, unused_val_num_classes = process_raw_to_input_and_labels(validation_raw, tag_to_integer_dictionary)\n",
    "unpadded_test, unpadded_test_labels, unused_test_dictionary, unused_test_num_classes = process_raw_to_input_and_labels(test_raw, tag_to_integer_dictionary)\n",
    "\n",
    "max_sentence_length = 0\n",
    "for sentence in unpadded_train:\n",
    "  if len(sentence) > max_sentence_length:\n",
    "    max_sentence_length = len(sentence)\n",
    "for sentence in unpadded_val:\n",
    "  if len(sentence) > max_sentence_length:\n",
    "    max_sentence_length = len(sentence)\n",
    "for sentence in unpadded_test:\n",
    "  if len(sentence) > max_sentence_length:\n",
    "    max_sentence_length = len(sentence)\n",
    "\n",
    "train, train_labels = pad_input_and_labels(unpadded_train, unpadded_train_labels, max_sentence_length)\n",
    "val, val_labels = pad_input_and_labels(unpadded_val, unpadded_val_labels, max_sentence_length)\n",
    "test, test_labels = pad_input_and_labels(unpadded_test, unpadded_test_labels, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "126eeaf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(train).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e67642f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15035, 51, 300)\n",
      "(3494, 51, 300)\n",
      "(3704, 51, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bcaf02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2e668052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 51, 300)]         0         \n",
      "                                                                 \n",
      " masking_1 (Masking)         (None, 51, 300)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 51, 32)            40576     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 51, 8)             264       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40840 (159.53 KB)\n",
      "Trainable params: 40840 (159.53 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape = (train.shape[1], train.shape[2]))\n",
    "x = layers.Masking(mask_value=0.0)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(16, return_sequences=True))(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c6331a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from seqeval.scheme import IOB1\n",
    "\n",
    "idx_to_label = {idx: label for label, idx in tag_to_integer_dictionary.items()}\n",
    "\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, train_data, train_labels, val_data, val_labels, mask_value=999):\n",
    "        super(F1ScoreCallback, self).__init__()\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.val_data = val_data\n",
    "        self.val_labels = val_labels\n",
    "        self.mask_value = mask_value\n",
    "#         self.best_f1 = -1\n",
    "#         self.patience = 5\n",
    "#         self.num_epochs_without_improvement = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Predictions for the training set\n",
    "        train_softmaxed_outputs = self.model.predict(self.train_data)\n",
    "        train_predicted_indices = np.argmax(train_softmaxed_outputs, axis=-1)\n",
    "\n",
    "        # Mask the training predictions and labels\n",
    "        train_mask = (self.train_labels != self.mask_value)\n",
    "        filtered_train_pred = train_predicted_indices[train_mask]\n",
    "        filtered_train_pred_labels = [[idx_to_label[index] for index in filtered_train_pred]]\n",
    "        filtered_train_true = self.train_labels[train_mask]\n",
    "        filtered_train_true_labels = [[idx_to_label[index] for index in filtered_train_true]]\n",
    "        \n",
    "\n",
    "        # Predictions for the validation set\n",
    "        val_softmaxed_outputs = self.model.predict(self.val_data)\n",
    "        val_predicted_indices = np.argmax(val_softmaxed_outputs, axis=-1)\n",
    "\n",
    "        # Mask the validation predictions and labels\n",
    "        val_mask = (self.val_labels != self.mask_value)\n",
    "        filtered_val_pred = val_predicted_indices[val_mask]\n",
    "        filtered_val_pred_labels = [[idx_to_label[index] for index in filtered_val_pred]]\n",
    "        filtered_val_true = self.val_labels[val_mask]\n",
    "        filtered_val_true_labels = [[idx_to_label[index] for index in filtered_val_true]]\n",
    "\n",
    "        # Calculate the metrics\n",
    "        train_precision = precision_score(filtered_train_true_labels, filtered_train_pred_labels, average='weighted', scheme=IOB1)\n",
    "        train_recall = recall_score(filtered_train_true_labels, filtered_train_pred_labels, average='weighted', scheme=IOB1)\n",
    "        train_f1 = f1_score(filtered_train_true_labels, filtered_train_pred_labels, average='weighted', scheme=IOB1)\n",
    "\n",
    "        val_precision = precision_score(filtered_val_true_labels, filtered_val_pred_labels, average='weighted', scheme=IOB1)\n",
    "        val_recall = recall_score(filtered_val_true_labels, filtered_val_pred_labels, average='weighted', scheme=IOB1)\n",
    "        val_f1 = f1_score(filtered_val_true_labels, filtered_val_pred_labels, average='weighted', scheme=IOB1)\n",
    "\n",
    "        # Print the metrics\n",
    "        print(f'\\nEpoch {epoch + 1}')\n",
    "        print(f'Training Precision: {train_precision:.4f} | Training Recall: {train_recall:.4f} | Training F1: {train_f1:.4f}')\n",
    "        print(f'Validation Precision: {val_precision:.4f} | Validation Recall: {val_recall:.4f} | Validation F1: {val_f1:.4f}')\n",
    "        \n",
    "#         # Update the best F1 score\n",
    "#         if val_f1 > self.best_f1:\n",
    "#             self.best_f1 = val_f1\n",
    "#             self.num_epochs_without_improvement = 0\n",
    "#         else:\n",
    "#             self.num_epochs_without_improvement += 1\n",
    "\n",
    "#         # Check if early stopping condition is met\n",
    "#         if self.num_epochs_without_improvement >= self.patience:\n",
    "#             self.model.stop_training = True\n",
    "#             print(\"Early stopping is triggered.\")\n",
    "\n",
    "        # Update the logs dictionary with the F1 scores\n",
    "        logs['train_f1'] = train_f1\n",
    "        logs['val_f1'] = val_f1\n",
    "        \n",
    "# Instantiate EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_f1', patience=5, mode='max', restore_best_weights=True)\n",
    "\n",
    "# Then, create an instance of the F1ScoreCallback\n",
    "f1_score_callback = F1ScoreCallback(train_data=train, train_labels=train_labels, val_data=val, val_labels=val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "837fc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss_function(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  mask = tf.cast(tf.not_equal(y_true, 999), tf.float32)\n",
    "  tf.print(\"y_true: \", y_true)\n",
    "  tf.print(\"mask: \", mask)\n",
    "  #tf.print(y_true)\n",
    "  tf.print(y_pred[0][0][:])\n",
    "  tf.print(len(y_pred))\n",
    "  tf.print(len(y_pred[0]))\n",
    "  tf.print(len(y_pred[0][0]))\n",
    "  loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "  loss *= mask\n",
    "  #tf.print(tf.reduce_sum(loss) / tf.reduce_sum(mask))\n",
    "  return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "  \"\"\"\n",
    "  # Create a mask to ignore the loss for 999 values in y_true\n",
    "  mask = tf.cast(tf.not_equal(y_true, 999), tf.float32)\n",
    "\n",
    "  # Replace the 999 values with a valid class index (e.g., 0)\n",
    "  y_true_masked = tf.where(tf.not_equal(y_true, 999), y_true, 0)\n",
    "\n",
    "  # Calculate the loss using the modified y_true\n",
    "  loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_masked, y_pred)\n",
    "\n",
    "  # Apply the mask to zero-out the loss for originally masked values\n",
    "  loss *= mask\n",
    "\n",
    "  # Return the mean loss only for the unmasked elements\n",
    "  return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72a41dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48eada6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "470/470 [==============================] - 10s 10ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghavrnair/opt/anaconda3/envs/CZ4045/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training Precision: 0.5772 | Training Recall: 0.4460 | Training F1: 0.4958\n",
      "Validation Precision: 0.5814 | Validation Recall: 0.4558 | Validation F1: 0.5030\n",
      "470/470 [==============================] - 40s 64ms/step - loss: 0.5787 - accuracy: 0.8495 - val_loss: 0.3188 - val_accuracy: 0.9147 - train_f1: 0.4958 - val_f1: 0.5030\n",
      "Epoch 2/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 10ms/step\n",
      "\n",
      "Epoch 2\n",
      "Training Precision: 0.7269 | Training Recall: 0.7146 | Training F1: 0.7178\n",
      "Validation Precision: 0.7269 | Validation Recall: 0.7201 | Validation F1: 0.7221\n",
      "470/470 [==============================] - 22s 48ms/step - loss: 0.2161 - accuracy: 0.9405 - val_loss: 0.1795 - val_accuracy: 0.9568 - train_f1: 0.7178 - val_f1: 0.7221\n",
      "Epoch 3/50\n",
      "470/470 [==============================] - 6s 12ms/step\n",
      "110/110 [==============================] - 1s 12ms/step\n",
      "\n",
      "Epoch 3\n",
      "Training Precision: 0.7661 | Training Recall: 0.7640 | Training F1: 0.7642\n",
      "Validation Precision: 0.7681 | Validation Recall: 0.7717 | Validation F1: 0.7696\n",
      "470/470 [==============================] - 23s 49ms/step - loss: 0.1526 - accuracy: 0.9591 - val_loss: 0.1411 - val_accuracy: 0.9645 - train_f1: 0.7642 - val_f1: 0.7696\n",
      "Epoch 4/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 4\n",
      "Training Precision: 0.7904 | Training Recall: 0.7909 | Training F1: 0.7903\n",
      "Validation Precision: 0.7892 | Validation Recall: 0.7980 | Validation F1: 0.7934\n",
      "470/470 [==============================] - 22s 46ms/step - loss: 0.1295 - accuracy: 0.9652 - val_loss: 0.1246 - val_accuracy: 0.9684 - train_f1: 0.7903 - val_f1: 0.7934\n",
      "Epoch 5/50\n",
      "470/470 [==============================] - 6s 14ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 5\n",
      "Training Precision: 0.8105 | Training Recall: 0.8136 | Training F1: 0.8116\n",
      "Validation Precision: 0.8055 | Validation Recall: 0.8151 | Validation F1: 0.8102\n",
      "470/470 [==============================] - 23s 50ms/step - loss: 0.1166 - accuracy: 0.9688 - val_loss: 0.1114 - val_accuracy: 0.9711 - train_f1: 0.8116 - val_f1: 0.8102\n",
      "Epoch 6/50\n",
      "470/470 [==============================] - 6s 12ms/step\n",
      "110/110 [==============================] - 1s 10ms/step\n",
      "\n",
      "Epoch 6\n",
      "Training Precision: 0.8256 | Training Recall: 0.8198 | Training F1: 0.8226\n",
      "Validation Precision: 0.8142 | Validation Recall: 0.8149 | Validation F1: 0.8145\n",
      "470/470 [==============================] - 23s 48ms/step - loss: 0.1073 - accuracy: 0.9712 - val_loss: 0.1042 - val_accuracy: 0.9717 - train_f1: 0.8226 - val_f1: 0.8145\n",
      "Epoch 7/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 7\n",
      "Training Precision: 0.8349 | Training Recall: 0.8332 | Training F1: 0.8340\n",
      "Validation Precision: 0.8250 | Validation Recall: 0.8288 | Validation F1: 0.8269\n",
      "470/470 [==============================] - 22s 46ms/step - loss: 0.0995 - accuracy: 0.9728 - val_loss: 0.0980 - val_accuracy: 0.9736 - train_f1: 0.8340 - val_f1: 0.8269\n",
      "Epoch 8/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 8\n",
      "Training Precision: 0.8432 | Training Recall: 0.8377 | Training F1: 0.8399\n",
      "Validation Precision: 0.8325 | Validation Recall: 0.8310 | Validation F1: 0.8313\n",
      "470/470 [==============================] - 22s 47ms/step - loss: 0.0944 - accuracy: 0.9742 - val_loss: 0.0941 - val_accuracy: 0.9740 - train_f1: 0.8399 - val_f1: 0.8313\n",
      "Epoch 9/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 9\n",
      "Training Precision: 0.8542 | Training Recall: 0.8445 | Training F1: 0.8492\n",
      "Validation Precision: 0.8418 | Validation Recall: 0.8376 | Validation F1: 0.8395\n",
      "470/470 [==============================] - 22s 46ms/step - loss: 0.0888 - accuracy: 0.9755 - val_loss: 0.0902 - val_accuracy: 0.9751 - train_f1: 0.8492 - val_f1: 0.8395\n",
      "Epoch 10/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 10\n",
      "Training Precision: 0.8616 | Training Recall: 0.8538 | Training F1: 0.8574\n",
      "Validation Precision: 0.8447 | Validation Recall: 0.8437 | Validation F1: 0.8440\n",
      "470/470 [==============================] - 22s 47ms/step - loss: 0.0839 - accuracy: 0.9767 - val_loss: 0.0865 - val_accuracy: 0.9762 - train_f1: 0.8574 - val_f1: 0.8440\n",
      "Epoch 11/50\n",
      "470/470 [==============================] - 6s 13ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 11\n",
      "Training Precision: 0.8655 | Training Recall: 0.8572 | Training F1: 0.8610\n",
      "Validation Precision: 0.8504 | Validation Recall: 0.8462 | Validation F1: 0.8481\n",
      "470/470 [==============================] - 23s 49ms/step - loss: 0.0803 - accuracy: 0.9778 - val_loss: 0.0851 - val_accuracy: 0.9767 - train_f1: 0.8610 - val_f1: 0.8481\n",
      "Epoch 12/50\n",
      "470/470 [==============================] - 6s 13ms/step\n",
      "110/110 [==============================] - 1s 12ms/step\n",
      "\n",
      "Epoch 12\n",
      "Training Precision: 0.8699 | Training Recall: 0.8641 | Training F1: 0.8667\n",
      "Validation Precision: 0.8508 | Validation Recall: 0.8489 | Validation F1: 0.8497\n",
      "470/470 [==============================] - 25s 52ms/step - loss: 0.0772 - accuracy: 0.9785 - val_loss: 0.0826 - val_accuracy: 0.9766 - train_f1: 0.8667 - val_f1: 0.8497\n",
      "Epoch 13/50\n",
      "470/470 [==============================] - 6s 13ms/step\n",
      "110/110 [==============================] - 2s 18ms/step\n",
      "\n",
      "Epoch 13\n",
      "Training Precision: 0.8794 | Training Recall: 0.8660 | Training F1: 0.8723\n",
      "Validation Precision: 0.8549 | Validation Recall: 0.8471 | Validation F1: 0.8508\n",
      "470/470 [==============================] - 26s 55ms/step - loss: 0.0737 - accuracy: 0.9794 - val_loss: 0.0817 - val_accuracy: 0.9769 - train_f1: 0.8723 - val_f1: 0.8508\n",
      "Epoch 14/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 14\n",
      "Training Precision: 0.8847 | Training Recall: 0.8725 | Training F1: 0.8783\n",
      "Validation Precision: 0.8589 | Validation Recall: 0.8526 | Validation F1: 0.8557\n",
      "470/470 [==============================] - 25s 53ms/step - loss: 0.0703 - accuracy: 0.9805 - val_loss: 0.0798 - val_accuracy: 0.9776 - train_f1: 0.8783 - val_f1: 0.8557\n",
      "Epoch 15/50\n",
      "470/470 [==============================] - 6s 13ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 15\n",
      "Training Precision: 0.8886 | Training Recall: 0.8744 | Training F1: 0.8813\n",
      "Validation Precision: 0.8572 | Validation Recall: 0.8498 | Validation F1: 0.8534\n",
      "470/470 [==============================] - 28s 59ms/step - loss: 0.0681 - accuracy: 0.9809 - val_loss: 0.0790 - val_accuracy: 0.9771 - train_f1: 0.8813 - val_f1: 0.8534\n",
      "Epoch 16/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 16\n",
      "Training Precision: 0.8933 | Training Recall: 0.8827 | Training F1: 0.8877\n",
      "Validation Precision: 0.8617 | Validation Recall: 0.8575 | Validation F1: 0.8595\n",
      "470/470 [==============================] - 23s 50ms/step - loss: 0.0655 - accuracy: 0.9818 - val_loss: 0.0781 - val_accuracy: 0.9778 - train_f1: 0.8877 - val_f1: 0.8595\n",
      "Epoch 17/50\n",
      "470/470 [==============================] - 6s 12ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 17\n",
      "Training Precision: 0.8964 | Training Recall: 0.8850 | Training F1: 0.8906\n",
      "Validation Precision: 0.8654 | Validation Recall: 0.8592 | Validation F1: 0.8623\n",
      "470/470 [==============================] - 23s 49ms/step - loss: 0.0626 - accuracy: 0.9824 - val_loss: 0.0765 - val_accuracy: 0.9782 - train_f1: 0.8906 - val_f1: 0.8623\n",
      "Epoch 18/50\n",
      "470/470 [==============================] - 6s 12ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 18\n",
      "Training Precision: 0.8957 | Training Recall: 0.8929 | Training F1: 0.8943\n",
      "Validation Precision: 0.8624 | Validation Recall: 0.8645 | Validation F1: 0.8633\n",
      "470/470 [==============================] - 22s 47ms/step - loss: 0.0607 - accuracy: 0.9833 - val_loss: 0.0764 - val_accuracy: 0.9786 - train_f1: 0.8943 - val_f1: 0.8633\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 6s 12ms/step\n",
      "110/110 [==============================] - 1s 12ms/step\n",
      "\n",
      "Epoch 19\n",
      "Training Precision: 0.8982 | Training Recall: 0.8859 | Training F1: 0.8915\n",
      "Validation Precision: 0.8601 | Validation Recall: 0.8541 | Validation F1: 0.8565\n",
      "470/470 [==============================] - 24s 51ms/step - loss: 0.0586 - accuracy: 0.9837 - val_loss: 0.0781 - val_accuracy: 0.9775 - train_f1: 0.8915 - val_f1: 0.8565\n",
      "Epoch 20/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 20\n",
      "Training Precision: 0.9083 | Training Recall: 0.8948 | Training F1: 0.9014\n",
      "Validation Precision: 0.8667 | Validation Recall: 0.8621 | Validation F1: 0.8643\n",
      "470/470 [==============================] - 22s 48ms/step - loss: 0.0567 - accuracy: 0.9843 - val_loss: 0.0749 - val_accuracy: 0.9788 - train_f1: 0.9014 - val_f1: 0.8643\n",
      "Epoch 21/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 21\n",
      "Training Precision: 0.9128 | Training Recall: 0.9011 | Training F1: 0.9069\n",
      "Validation Precision: 0.8655 | Validation Recall: 0.8624 | Validation F1: 0.8639\n",
      "470/470 [==============================] - 22s 46ms/step - loss: 0.0549 - accuracy: 0.9849 - val_loss: 0.0744 - val_accuracy: 0.9786 - train_f1: 0.9069 - val_f1: 0.8639\n",
      "Epoch 22/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 22\n",
      "Training Precision: 0.9069 | Training Recall: 0.9010 | Training F1: 0.9038\n",
      "Validation Precision: 0.8647 | Validation Recall: 0.8655 | Validation F1: 0.8650\n",
      "470/470 [==============================] - 22s 47ms/step - loss: 0.0536 - accuracy: 0.9853 - val_loss: 0.0756 - val_accuracy: 0.9786 - train_f1: 0.9038 - val_f1: 0.8650\n",
      "Epoch 23/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 23\n",
      "Training Precision: 0.9181 | Training Recall: 0.9036 | Training F1: 0.9107\n",
      "Validation Precision: 0.8663 | Validation Recall: 0.8614 | Validation F1: 0.8638\n",
      "470/470 [==============================] - 22s 46ms/step - loss: 0.0517 - accuracy: 0.9857 - val_loss: 0.0738 - val_accuracy: 0.9787 - train_f1: 0.9107 - val_f1: 0.8638\n",
      "Epoch 24/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 24\n",
      "Training Precision: 0.9157 | Training Recall: 0.9027 | Training F1: 0.9090\n",
      "Validation Precision: 0.8700 | Validation Recall: 0.8641 | Validation F1: 0.8668\n",
      "470/470 [==============================] - 22s 47ms/step - loss: 0.0499 - accuracy: 0.9862 - val_loss: 0.0764 - val_accuracy: 0.9785 - train_f1: 0.9090 - val_f1: 0.8668\n",
      "Epoch 25/50\n",
      "470/470 [==============================] - 5s 12ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 25\n",
      "Training Precision: 0.9234 | Training Recall: 0.9110 | Training F1: 0.9171\n",
      "Validation Precision: 0.8681 | Validation Recall: 0.8646 | Validation F1: 0.8663\n",
      "470/470 [==============================] - 23s 49ms/step - loss: 0.0485 - accuracy: 0.9866 - val_loss: 0.0739 - val_accuracy: 0.9789 - train_f1: 0.9171 - val_f1: 0.8663\n",
      "Epoch 26/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 26\n",
      "Training Precision: 0.9235 | Training Recall: 0.9131 | Training F1: 0.9182\n",
      "Validation Precision: 0.8706 | Validation Recall: 0.8651 | Validation F1: 0.8677\n",
      "470/470 [==============================] - 22s 46ms/step - loss: 0.0470 - accuracy: 0.9871 - val_loss: 0.0743 - val_accuracy: 0.9786 - train_f1: 0.9182 - val_f1: 0.8677\n",
      "Epoch 27/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 27\n",
      "Training Precision: 0.9272 | Training Recall: 0.9164 | Training F1: 0.9217\n",
      "Validation Precision: 0.8702 | Validation Recall: 0.8667 | Validation F1: 0.8684\n",
      "470/470 [==============================] - 22s 46ms/step - loss: 0.0460 - accuracy: 0.9873 - val_loss: 0.0730 - val_accuracy: 0.9790 - train_f1: 0.9217 - val_f1: 0.8684\n",
      "Epoch 28/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 28\n",
      "Training Precision: 0.9291 | Training Recall: 0.9130 | Training F1: 0.9208\n",
      "Validation Precision: 0.8757 | Validation Recall: 0.8665 | Validation F1: 0.8708\n",
      "470/470 [==============================] - 22s 47ms/step - loss: 0.0443 - accuracy: 0.9879 - val_loss: 0.0771 - val_accuracy: 0.9789 - train_f1: 0.9208 - val_f1: 0.8708\n",
      "Epoch 29/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 29\n",
      "Training Precision: 0.9353 | Training Recall: 0.9187 | Training F1: 0.9269\n",
      "Validation Precision: 0.8704 | Validation Recall: 0.8626 | Validation F1: 0.8664\n",
      "470/470 [==============================] - 22s 47ms/step - loss: 0.0428 - accuracy: 0.9885 - val_loss: 0.0758 - val_accuracy: 0.9788 - train_f1: 0.9269 - val_f1: 0.8664\n",
      "Epoch 30/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 30\n",
      "Training Precision: 0.9349 | Training Recall: 0.9257 | Training F1: 0.9301\n",
      "Validation Precision: 0.8686 | Validation Recall: 0.8670 | Validation F1: 0.8677\n",
      "470/470 [==============================] - 23s 49ms/step - loss: 0.0415 - accuracy: 0.9888 - val_loss: 0.0750 - val_accuracy: 0.9789 - train_f1: 0.9301 - val_f1: 0.8677\n",
      "Epoch 31/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 31\n",
      "Training Precision: 0.9380 | Training Recall: 0.9287 | Training F1: 0.9333\n",
      "Validation Precision: 0.8696 | Validation Recall: 0.8680 | Validation F1: 0.8687\n",
      "470/470 [==============================] - 22s 48ms/step - loss: 0.0403 - accuracy: 0.9889 - val_loss: 0.0742 - val_accuracy: 0.9786 - train_f1: 0.9333 - val_f1: 0.8687\n",
      "Epoch 32/50\n",
      "470/470 [==============================] - 5s 11ms/step\n",
      "110/110 [==============================] - 1s 11ms/step\n",
      "\n",
      "Epoch 32\n",
      "Training Precision: 0.9411 | Training Recall: 0.9243 | Training F1: 0.9326\n",
      "Validation Precision: 0.8746 | Validation Recall: 0.8660 | Validation F1: 0.8702\n",
      "470/470 [==============================] - 22s 48ms/step - loss: 0.0390 - accuracy: 0.9895 - val_loss: 0.0760 - val_accuracy: 0.9788 - train_f1: 0.9326 - val_f1: 0.8702\n",
      "Epoch 33/50\n",
      "470/470 [==============================] - 5s 12ms/step\n",
      "110/110 [==============================] - 1s 13ms/step\n",
      "\n",
      "Epoch 33\n",
      "Training Precision: 0.9448 | Training Recall: 0.9314 | Training F1: 0.9380\n",
      "Validation Precision: 0.8705 | Validation Recall: 0.8667 | Validation F1: 0.8685\n",
      "470/470 [==============================] - 24s 51ms/step - loss: 0.0379 - accuracy: 0.9896 - val_loss: 0.0755 - val_accuracy: 0.9787 - train_f1: 0.9380 - val_f1: 0.8685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fcd95af7610>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=adam_optimizer, loss=masked_loss_function, metrics=[\"accuracy\"])\n",
    "#model.fit(train, train_labels, batch_size=32, epochs=2)\n",
    "model.fit(train, train_labels, batch_size=32, epochs=50, validation_data=(val, val_labels), callbacks=[f1_score_callback, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f5f0f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 10s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "# Testing layer outputs\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Create a new model that will return the outputs from all layers:\n",
    "layer_outputs = [layer.output for layer in model.layers]  # Exclude the Input layer if necessary\n",
    "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# Get the outputs for an input:\n",
    "all_layer_activations = activation_model.predict(train)\n",
    "\n",
    "# Now iterate over the outputs and check for NaNs:\n",
    "for layer_activation in all_layer_activations:\n",
    "    # Check if the activation contains NaNs\n",
    "    if np.isnan(layer_activation).any():\n",
    "        print(\"NaNs detected\")\n",
    "\n",
    "# If you want to check a particular layer by name, you can do:\n",
    "for layer, activation in zip(model.layers, all_layer_activations):\n",
    "    if np.isnan(activation).any():\n",
    "        print(f\"NaN detected in layer: {layer.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76fb1db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 1s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "test_softmaxed_outputs = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506afbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
